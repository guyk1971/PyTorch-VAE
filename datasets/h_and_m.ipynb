{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Fashion Dataset\n",
    "a notebook to explore the [H&M Fashion dataset](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations) in order to prepare a `Dataset` class and `Dataloader` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from glob import glob as Glob\n",
    "import json\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.uname()[1]=='guy-x':\n",
    "    dataset_path = '/home/guy/sd1tb/datasets/h-and-m-personalized-fashion-recommendations'\n",
    "else:   # assuming gpu15\n",
    "    dataset_path = '/data/users/gkoren2/datasets/h-and-m-personalized-fashion-recommendation'       # gpu15\n",
    "\n",
    "print(f\"assuming we're on {os.uname()[1]} so data is in {dataset_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1 to handle images \n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def image_grid2(imgs, nrows = 1, ncols=1,figsize=(16, 16)):\n",
    "    \"\"\"Plot a dictionary of Image objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : dict of Image objects\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows,figsize=figsize)\n",
    "    for ind,aid in enumerate(imgs):\n",
    "        axeslist.ravel()[ind].imshow(imgs[aid])\n",
    "        axeslist.ravel()[ind].set_title(aid)\n",
    "        axeslist.ravel()[ind].set_axis_off()\n",
    "    plt.tight_layout() # optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "Description provided by the site:\n",
    "- images/ - a folder of images corresponding to each article_id; images are placed in subfolders starting with the first three digits of the article_id; note, not all article_id values have a corresponding image.\n",
    "- articles.csv - detailed metadata for each article_id available for purchase\n",
    "- customers.csv - metadata for each customer_id in dataset\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "- transactions_train.csv - the training data, consisting of the purchases each customer for each date, as well as additional information. Duplicate rows correspond to multiple purchases of the same item. Your task is to predict the article_ids each customer will purchase during the 7-day period immediately after the training data period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2 to handle images\n",
    "import cv2\n",
    "def plot_figures(figures, nrows = 1, ncols=1,figsize=(8, 8)):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows,figsize=figsize)\n",
    "    for ind,title in enumerate(figures):\n",
    "        axeslist.ravel()[ind].imshow(cv2.cvtColor(figures[title], cv2.COLOR_BGR2RGB))\n",
    "        axeslist.ravel()[ind].set_title(title)\n",
    "        axeslist.ravel()[ind].set_axis_off()\n",
    "    plt.tight_layout() # optional\n",
    "    \n",
    "def img_path(img):\n",
    "    return dataset_path+\"/images/\"+img\n",
    "\n",
    "def load_image(img, resized_fac = 0.1):\n",
    "    img     = cv2.imread(img_path(img))\n",
    "    w, h, _ = img.shape\n",
    "    resized = cv2.resize(img, (int(h*resized_fac), int(w*resized_fac)), interpolation = cv2.INTER_AREA)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = pd.read_csv(dataset_path + \"/articles.csv\",on_bad_lines='skip')\n",
    "# df['image'] = df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\n",
    "# df = df.reset_index(drop=True)\n",
    "print(adf.shape)\n",
    "adf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure how many unique values in each column:\n",
    "{c:len(adf[c].value_counts()) for c in adf.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['perceived_colour_master_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['perceived_colour_value_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['colour_group_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['graphical_appearance_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['product_group_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_csv(dataset_path + \"/customers.csv\",on_bad_lines='skip')\n",
    "# df['image'] = df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\n",
    "# df = df.reset_index(drop=True)\n",
    "print(cdf.shape)\n",
    "cdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdf = pd.read_csv(dataset_path + \"/transactions_train.csv\",on_bad_lines='skip')\n",
    "# df['image'] = df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\n",
    "# df = df.reset_index(drop=True)\n",
    "print(trdf.shape)\n",
    "trdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn=[i for i in Glob(f'{dataset_path}/images/**/*.jpg',recursive=True)]\n",
    "len(img_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = pd.read_csv(dataset_path + \"/articles.csv\",on_bad_lines='skip')\n",
    "# adf['img_fn'] = adf.apply(lambda row: dataset_path+'/images/0'+str(row['article_id'])[:2]+'/0'+str(row['article_id']) + \".jpg\", axis=1)\n",
    "def get_imgs_by_aid(aid_list,size=(512,512)):\n",
    "    aid_df = adf.loc[adf['article_id'].isin(aid_list),:]\n",
    "    ifn={str(a):f for f in img_fn for a in aid_list if str(a) in f }\n",
    "    imgs = {os.path.splitext(os.path.basename(a))[0]:Image.open(ifn[a]) for a in ifn}\n",
    "    [i.thumbnail(size) for i in imgs.values()]\n",
    "    print(aid_df)\n",
    "    image_grid2(imgs,1,len(imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_list=adf.loc[10:15,'article_id'].values\n",
    "get_imgs_by_aid(aid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_square_img(img,size):\n",
    "    \n",
    "    old_size = img.size  # old_size[0] is in (width, height) format\n",
    "\n",
    "    ratio = float(size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # use thumbnail() or resize() method to resize the input image\n",
    "\n",
    "    # thumbnail is a in-place operation\n",
    "\n",
    "    # im.thumbnail(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    im = img.resize(new_size, Image.ANTIALIAS)\n",
    "    # create a new image and paste the resized on it\n",
    "\n",
    "    new_im = Image.new(\"RGB\", (size, size))\n",
    "    new_im.paste(im, ((size-new_size[0])//2,\n",
    "                        (size-new_size[1])//2))\n",
    "    return new_im    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def create_square_img2(img,size):\n",
    "    \n",
    "    old_size = img.size  # old_size[0] is in (width, height) format\n",
    "\n",
    "    ratio = float(size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # use thumbnail() or resize() method to resize the input image\n",
    "\n",
    "    # thumbnail is a in-place operation\n",
    "\n",
    "    # im.thumbnail(new_size, Image.ANTIALIAS)\n",
    "    im = img.resize(new_size, Image.ANTIALIAS)\n",
    "    # create a new image and paste the resized on it\n",
    "\n",
    "    im = np.array(im)\n",
    "    im = cv2.copyMakeBorder(im,(size-new_size[1])//2,(size-new_size[1])//2,(size-new_size[0])//2,(size-new_size[0])//2,cv2.BORDER_REPLICATE)\n",
    "\n",
    "    new_im = Image.fromarray(im)\n",
    "    return new_im    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs={os.path.splitext(os.path.basename(im))[0]:Image.open(im) for im in img_fn[5:9]}\n",
    "[imgs[im].thumbnail((256,256)) for im in imgs]\n",
    "# image_grid2(imgs,1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[imgs[im].size for im in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_square_img2(imgs['0607421011'],256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsz = [Image.open(im).size for im in img_fn]\n",
    "pd.Series(imsz).value_counts()\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find image with a given size\n",
    "im=[img_fn[i] for i in range(len(img_fn)) if imsz[i] == (480,640) ]\n",
    "Image.open(im[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find images with a given id\n",
    "im=[img_fn[i] for i in range(len(img_fn)) if imsz[i] == (480,640) ]\n",
    "Image.open(im[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize the images with a limit on each axis size, while maintaining aspect ratio\n",
    "size=(256,256)\n",
    "imgs = [Image.open(im) for im in img_fn[100:110]]\n",
    "[i.thumbnail(size) for i in imgs]\n",
    "[im.size for im in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(imgs,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of a dictionary of (title, images)\n",
    "figures = {'im'+str(i): load_image(row.image) for i, row in df.sample(6).iterrows()}\n",
    "# plot of the images in a figure, with 2 rows and 3 columns\n",
    "plot_figures(figures, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "The goal is to prepare a dataset class and dataloader that we can use to train the VAE.\n",
    "we can use the code from the `Text2Human` repo\n",
    "\n",
    "here's another [repo](https://github.com/ihciah/deep-fashion-retrieval) that uses the datasets.\n",
    "\n",
    "and here's another repo from the authors of DeepFashion: [mmfashion](https://github.com/open-mmlab/mmfashion)\n",
    "in that repo, look at the [In_shop.py](https://github.com/open-mmlab/mmfashion/blob/master/mmfashion/datasets/In_shop.py) file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Normalization\n",
    "in order to normalize the dataset, we could either use the pytorch official normalization statistics as derived from ImageNet (see [this discussion](https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2)):  \n",
    "`mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])` \n",
    "\n",
    "but we might want to compute the statistics by ourselves as the images in this dataset are not that natural. they all have a clean background.\n",
    "for that, lets define the dataset class without the normalization and then compute its stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = T.Compose([\n",
    "#     T.ToPILImage(),\n",
    "    #---------comment out data augmentation------\n",
    "    # T.ColorJitter(brightness=(0.8,1.2), \n",
    "    # contrast=(0.8,1.2), \n",
    "    # saturation=(0.8,1.2), \n",
    "    # hue=0.25),\n",
    "    # T.RandomAffine(5, translate=(0.01,0.1)),\n",
    "    #---------------------------------------------\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "    #             std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_tfms = T.Compose([\n",
    "#     T.ToPILImage(),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "    #             std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "class DeepFashion(Dataset):\n",
    "    \"\"\"DeepFashion dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        fpath = self.files[ix]\n",
    "#         clss = fname(parent(fpath))  # todo: read the label/attributes\n",
    "        img = read_image(fpath)/255.0\n",
    "        # return a dummy label at this stage as it doesnt matter\n",
    "        return img,1\n",
    "\n",
    "    def choose(self):\n",
    "        return self[np.random.randint(len(self))]\n",
    "\n",
    "    # def collate_fn(self, batch):\n",
    "    #     imgs, classes = list(zip(*batch))\n",
    "    #     if self.transform:\n",
    "    #         imgs = [self.transform(img)[None] for img in imgs]\n",
    "    #     classes = [torch.tensor([id2int[clss]]) for clss in classes]\n",
    "    #     imgs, classes = [torch.cat(i).to(device) for i in [imgs, classes]]\n",
    "    #     return imgs, classes\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        imgs, classes = list(zip(*batch))\n",
    "        if self.transform:\n",
    "            imgs = [self.transform(img)[None] for img in imgs]\n",
    "        return imgs\n",
    "\n",
    "\n",
    "dataset = DeepFashion(img_fn,trn_tfms)\n",
    "loader = DataLoader(dataset,batch_size=10,num_workers=0,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(loader))\n",
    "b,c,h,w=next(iter(loader))[0].shape\n",
    "print(b,c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(batch[0][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once we have the dataset defined we can compute the stats as follows ([reference](https://discuss.pytorch.org/t/computing-the-mean-and-std-of-dataset/34949))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset statistics. to be used for normalizing the dataset\n",
    "# h,w=next(iter(loader))[0].shape[2:]\n",
    "mean = 0.0\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0) \n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "mean = mean / len(loader.dataset)\n",
    "\n",
    "var = 0.0\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    var += ((images - mean.unsqueeze(1))**2).sum([0,2])\n",
    "std = torch.sqrt(var / (len(loader.dataset)*h*w))\n",
    "\n",
    "\n",
    "print(f'mean:{mean},std:{std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class definition\n",
    "now that we have the dataset statistics we can create a dataset that normalizes appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(256),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=[0.8323, 0.8108, 0.8040], \n",
    "    #             std=[0.2332, 0.2500, 0.2564]),\n",
    "])\n",
    "\n",
    "val_tfms = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(256),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=[0.8323, 0.8108, 0.8040], \n",
    "    #             std=[0.2332, 0.2500, 0.2564]),\n",
    "])\n",
    "\n",
    "class DeepFashion(Dataset):\n",
    "    \"\"\"DeepFashion dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path, split='train',transform=None):\n",
    "        self.files=[i for i in Glob(f'{dataset_path}/Img/**/*.jpg',recursive=True)] \n",
    "        # read item attributes:\n",
    "        # the attribute can be the cloth attribute or it can be viewing angle or type of item (based on the folder)\n",
    "        # in the following example, we're taking the cloth attributes (463 attributes)\n",
    "        anno_filename = os.path.join(dataset_path,'Anno','attributes','list_attr_items.txt')\n",
    "        with open(anno_filename,'r') as f:\n",
    "            attr_items=f.read().splitlines()\n",
    "        attr_items=attr_items[2:]\n",
    "        attr_items=[i.split() for i in attr_items]\n",
    "        self.attr_items = {i[0]:[int(int(a)>0) for a in i[1:]] for i in attr_items}\n",
    "        # get attributes names\n",
    "        anno_filename = os.path.join(dataset_path,'Anno','attributes','list_attr_cloth.txt')\n",
    "        with open(anno_filename,'r') as f:\n",
    "            attr_cloth=f.read().splitlines()\n",
    "        self.attr_names = attr_cloth[2:]\n",
    "\n",
    "        # split the dataset and throw the irrelevant part\n",
    "        train_fraction=0.8\n",
    "        train_size = int(train_fraction* len(self.attr_items.keys()))\n",
    "\n",
    "        if split=='train':\n",
    "            self.files = self.files[:train_size]\n",
    "        else:\n",
    "            self.files = self.files[train_size:]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        fpath = self.files[ix]\n",
    "        lbl=self.attr_items[fpath.split('/')[-2]]\n",
    "        img = read_image(fpath)/255.0\n",
    "        return img,lbl\n",
    "\n",
    "    def choose(self):\n",
    "        return self[np.random.randint(len(self))]\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        imgs, attrs = list(zip(*batch))\n",
    "        if self.transform:\n",
    "            imgs = [self.transform(img)[None] for img in imgs]\n",
    "        else:\n",
    "            imgs = [img[None] for img in imgs]\n",
    "        attrs=[torch.tensor(a)[None] for a in attrs]\n",
    "        imgs,attrs = [torch.cat(i) for i in [imgs,attrs]]\n",
    "        return imgs,attrs\n",
    "\n",
    "dataset = DeepFashion(dataset_path,'train')\n",
    "loader = DataLoader(dataset,batch_size=10,num_workers=0,shuffle=False,collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(loader))\n",
    "b,c,h,w=batch[0].shape\n",
    "print(b,c,h,w)\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(batch[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CelebA\n",
    "this is the dataset considered as a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clba_path = '/home/guy/sd1tb/datasets/CelebA/celeba/img_align_celeba'\n",
    "# clba_path = '/data/users/gkoren2/datasets/celeba/celeba/img_align_celeba'   # gpu15\n",
    "imgs_filenames=os.listdir(clba_path)\n",
    "len(imgs_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract image sizes and make sure they all at sime size\n",
    "imsz = [Image.open(os.path.join(clba_path,im)).size for im in imgs_filenames]\n",
    "np.all([s==imsz[0] for s in imsz])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the alignment by displaying the average of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_indxs = [i for i in range(1000)]\n",
    "avg_img = np.zeros_like(Image.open(os.path.join(clba_path,imgs_filenames[0])),dtype=np.float64)\n",
    "for i,idx in enumerate(img_indxs):\n",
    "    img=Image.open(os.path.join(clba_path,imgs_filenames[idx]))\n",
    "    avg_img+=np.asarray(img)\n",
    "avg_img = avg_img/len(img_indxs)\n",
    "Image.fromarray(np.uint8(avg_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, we see that the images are pretty much aligned. this is something that should predict the successfulness of the VAE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make a gif from the images:\n",
    "img_indxs = [i for i in range(1000)]\n",
    "imgs = (Image.open(os.path.join(clba_path,imgs_filenames[i])) for i in img_indxs)\n",
    "img = next(imgs)  # extract first image from iterator\n",
    "img.save(fp='celeba.gif', format='GIF', append_images=imgs,save_all=True, duration=20, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load CelebA dataset class\n",
    "Lets import the dataset as it is done in the VAE s.t. we understand how to prepare `DeepFashion` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CelebA\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms as T\n",
    "# clba_path = '/data/users/gkoren2/datasets/celeba/'   # gpu15\n",
    "clba_path = '/home/guy/sd1tb/datasets/CelebA/'\n",
    "patch_size=256\n",
    "trn_tfms = T.Compose([      # copied from dataset.py \n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.CenterCrop(148),\n",
    "    T.Resize(patch_size),\n",
    "    T.ToTensor(),\n",
    "    ])\n",
    "\n",
    "clba = CelebA(clba_path, split='train',transform=trn_tfms,download=False)\n",
    "\n",
    "dir(clba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(clba,'collate_fn',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clba_loader = data.DataLoader(clba,batch_size=10,num_workers=0,shuffle=False)\n",
    "clba_batch=next(iter(clba_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clba_batch[0].shape)       # the images tensor\n",
    "print(clba_batch[1].shape)       # the labels (attributes) tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clba.attr_names))\n",
    "clba.attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clba_batch[0].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like the only normalization they have applied is scaling s.t. the value of the pixels are in [0,1]. they havent normalized it to `mean=0 , stdev=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(clba_batch[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clba_batch[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clba_batch[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFashion dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Shop Clothes Retrieval\n",
    "\n",
    "- Images (Img/img.zip):   52,712 in-shop clothes images (~200,000 cross-pose/scale pairs).\n",
    "    - Images are centered and resized to 256*256;\n",
    "    - The aspect ratios of original images are kept unchanged\n",
    "- Bounding Box Annotations (Anno/list_bbox_inshop.txt)\n",
    "- Fashion Landmark Annotations (Anno/list_landmarks_inshop.txt)\n",
    "- Item Annotations (Anno/list_item_inshop.txt)\n",
    "- Description Annotations (Anno/list_description_inshop.json)\n",
    "- Attribute Annotations (Anno/attributes/list_attr_cloth.txt & Anno/attributes/list_attr_items.txt & Anno/attributes/list_color_cloth.txt)\n",
    "- Segmentation Mask Annotations (Anno/segmentation/DeepFashion_instances_train.json & Anno/segmentation/DeepFashion_instances_query.json & Anno/segmentation/DeepFashion_instances_gallery.json)\n",
    "- Dense Pose Annotations (Anno/densepose/img_iuv.zip)\n",
    "- Evaluation Partitions (Eval/list_eval_partition.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/users/gkoren2/datasets/DeepFashion/In-shop-Clothes-Retrieval/In-shop Clothes Retrieval Benchmark' # gpu15\n",
    "# benchmarks = [os.path.basename(bm) for bm in Glob(dataset_path+'/*') if os.path.isdir(bm)]\n",
    "# benchmarks\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_filename = os.path.join(dataset_path,'Anno','list_item_inshop.txt')\n",
    "with open(anno_filename,'r') as f:\n",
    "    item_list=f.read().splitlines()\n",
    "\n",
    "len(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_filename = os.path.join(dataset_path,'Anno','list_description_inshop.json')\n",
    "with open(anno_filename,'r') as f:\n",
    "    item_desc = json.load(f)\n",
    "desc_df=pd.DataFrame(item_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df.item.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df.loc[desc_df['item']=='id_00000001',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_filename = os.path.join(dataset_path,'Anno','attributes','list_attr_cloth.txt')\n",
    "with open(anno_filename,'r') as f:\n",
    "    attr_cloth=f.read().splitlines()\n",
    "attr_cloth = attr_cloth[2:]\n",
    "len(attr_cloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_cloth[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_filename = os.path.join(dataset_path,'Anno','attributes','list_attr_items.txt')\n",
    "with open(anno_filename,'r') as f:\n",
    "    attr_items=f.read().splitlines()\n",
    "attr_items=attr_items[2:]\n",
    "attr_items=[i.split() for i in attr_items]\n",
    "attr_items = {i[0]:[int(int(a)>0) for a in i[1:]] for i in attr_items}\n",
    "len(attr_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(attr_items['id_00000013']))\n",
    "attr_items['id_00000013']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. The order of attribute labels accords with the order of attribute names;\n",
    "2. In attribute labels, \"1\" represents positive while \"-1\" represents negative, '0' represents unknown;\n",
    "3. Attribute prediction is treated as a multi-label tagging problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_item_desc={i[0]:[attr_cloth[ai] for ai in range(len(attr_cloth)) if i[ai+1]=='1'] for i in attr_items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_item_desc['id_00000013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_filename = os.path.join(dataset_path,'Anno','attributes','list_color_cloth.txt')\n",
    "with open(anno_filename,'r') as f:\n",
    "    color_cloth=f.read().splitlines()\n",
    "\n",
    "len(color_cloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_cloth[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_part_filename = os.path.join(dataset_path,'Eval','list_eval_partition.txt')\n",
    "with open(eval_part_filename,'r') as f:\n",
    "    eval_part_list=f.read().splitlines()\n",
    "eval_part_list=eval_part_list[2:]\n",
    "eval_part_list=[i.split() for i in eval_part_list]\n",
    "eval_part_list = {i[1]:i[2] for i in eval_part_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(eval_part_list.values()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epl0=eval_part_list[2]\n",
    "epl0.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tree \"{os.path.join(dataset_path,'Img/img')}\" -L 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn=[i for i in Glob(f'{dataset_path}/Img/**/*.jpg',recursive=True)] #  if 'side' in i]\n",
    "len(img_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base name\n",
    "img_bn=[s.split('img')[1] for s in img_fn]\n",
    "len(img_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in img_fn if 'id_00003321' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tree \"{os.path.join(os.path.dirname(dataset_path),'img_highres')}\" -L 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hres_fn = [i for i in Glob(f\"{os.path.join(os.path.dirname(dataset_path),'img_highres')}/**/*.jpg\",recursive=True)]\n",
    "len(img_hres_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hres_fn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hres_bn=[s.split('img_highres')[1] for s in img_hres_fn]\n",
    "len(img_hres_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comapre the names - make sure we have exact copy of each image as highres\n",
    "np.all(np.array(img_bn)==np.array(img_hres_bn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see an image and its highres variant\n",
    "img_id = 2345\n",
    "img = Image.open(img_fn[img_id])\n",
    "img_hres=Image.open(img_hres_fn[img_id])\n",
    "print(f'{img.size} , {img_hres.size}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll ignore the high res images. look only at the low res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get item images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn=[i for i in Glob(f'{dataset_path}/Img/**/*.jpg',recursive=True)] #  if 'side' in i]\n",
    "img_hres_fn = [i for i in Glob(f\"{os.path.join(os.path.dirname(dataset_path),'img_highres')}/**/*.jpg\",recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_images(item_id,hres=False):\n",
    "    img_fns=img_hres_fn if hres else img_fn\n",
    "    img_filenames = [f for f in img_fns if item_id in f]\n",
    "    if len(img_filenames)==0:\n",
    "        print(f'item {item_id} not found')\n",
    "        return None\n",
    "    else:\n",
    "        imgs = {os.path.basename(f):Image.open(f) for f in img_filenames}\n",
    "        print(f'found {len(imgs)} for item {item_id}')\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = get_item_images('id_00000002',False)\n",
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(list(imgs.values()),2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image stats\n",
    "img_indxs = [i for i in range(len(img_fn))]\n",
    "avg_img = np.zeros_like(Image.open(img_fn[0]),dtype=np.float64)\n",
    "for i,idx in enumerate(img_indxs):\n",
    "    img=Image.open(img_fn[idx])\n",
    "    avg_img+=np.asarray(img)\n",
    "avg_img = avg_img/len(img_indxs)\n",
    "# Image.fromarray(np.uint8(avg_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn[0].split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_img[:,:,0].mean()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai1=avg_img.reshape([-1,3]).mean(axis=0)/255\n",
    "ai1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai1-np.array([0.485, 0.456, 0.406])/np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category and Attribute Prediction\n",
    "\n",
    "- Images (Img/img.zip):  289,222 diverse clothes images. \n",
    "    - The long side of images are resized to 300;\n",
    "    - The aspect ratios of original images are kept unchanged.\n",
    "- Bounding Box Annotations (Anno/list_bbox.txt) bounding box labels. \n",
    "- Fashion Landmark Annotations (Anno/list_landmarks.txt) fashion landmark labels\n",
    "- Category Annotations (Anno/list_category_cloth.txt & Anno/list_category_img.txt) clothing category labels. \n",
    "- Attribute Annotations (Anno/list_attr_cloth.txt & Anno/list_attr_img.txt) clothing attribute labels. \n",
    "- Evaluation Partitions (Eval/list_eval_partition.txt) image names for training, validation and testing set respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/users/gkoren2/datasets/DeepFashion/Category-and-Attribute-Prediction' # gpu15\n",
    "# benchmarks = [os.path.basename(bm) for bm in Glob(dataset_path+'/*') if os.path.isdir(bm)]\n",
    "# benchmarks\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn=[i for i in Glob(f'{dataset_path}/img/**/*.jpg',recursive=True)] #  if 'side' in i]\n",
    "img_hres_fn = [i for i in Glob(f\"{os.path.join(dataset_path,'img_highres')}/**/*.jpg\",recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(img_fn))\n",
    "print(len(img_hres_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(os.path.dirname(img_fn[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=[Image.open(f) for f in img_fn[100:110]]\n",
    "image_grid(imgs,2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/users/gkoren2/datasets/DeepFashion/In-shop-Clothes-Retrieval' # gpu15\n",
    "# dataset_path = '/home/guy/sd1tb/datasets/deep_fashion/DeepFashion' # guy-x\n",
    "benchmarks = [os.path.basename(bm) for bm in Glob(dataset_path+'/*') if os.path.isdir(bm)]\n",
    "benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn=[i for i in Glob(f'{dataset_path}/{benchmarks[0]}/Img/**/*.jpg',recursive=True)] #  if 'side' in i]\n",
    "len(img_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsz = [Image.open(im).size for im in img_fn]\n",
    "df=pd.DataFrame(imsz,columns=['x','y'])\n",
    "df['new_col'] = list(zip(df.x, df.y))\n",
    "df['img_fn']=img_fn\n",
    "df['new_col'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the smallest image in the dataset\n",
    "# img=Image.open(df.loc[df['x']==70,'img_fn'])\n",
    "img=Image.open(df.loc[df['x']==df['x'].min(),'img_fn'].values[0])\n",
    "print(img.size)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x'].sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=[Image.open(m) for m in img_fn[200:225]]\n",
    "image_grid(imgs,5,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the average image \n",
    "img_indxs = [i for i in range(len(img_fn))]\n",
    "avg_img = np.zeros_like(Image.open(img_fn[0]),dtype=np.float64)\n",
    "for i,idx in enumerate(img_indxs):\n",
    "    img=Image.open(img_fn[idx])\n",
    "    avg_img+=np.asarray(img)\n",
    "avg_img = avg_img/len(img_indxs)\n",
    "Image.fromarray(np.uint8(avg_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the average image looks poor. it would be interesting to see whether the VAE can capture the variance in the dataset and get farther from the average image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, lets start with the `In-Shop-Clothes-Retreival-Benchmark` dataset and try to find a disentangled representation using VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/users/gkoren2/datasets/DeepFashion/In-shop-Clothes-Retrieval/In-shop Clothes Retrieval Benchmark' # gpu15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "things we'll need to do:\n",
    "- transform to tensor\n",
    "- normalize the dataset\n",
    "\n",
    "we wont do any further adjustments or augmentations at this stage.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis_ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d4a6bc45846aab54e7d9593687b4f0ac37d84ff54eabfefe731b4bf11ea2c68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
