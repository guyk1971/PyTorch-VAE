{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion Dataset\n",
    "a notebook to explore the DeepFashion dataset in order to prepare a `Dataset` class and `Dataloader` \n",
    "\n",
    "Note that the data used for Text2Human project was processed vs the original DeepFashion-MultiModal thus we might want to use this dataset, as the images are already aligned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/intel/bin/bash: /home/gkoren2/miniconda3/envs/ptvae/lib/libtinfo.so.6: no version information available (required by /usr/intel/bin/bash)\n",
      "/home/gkoren2/miniconda3/envs/ptvae/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from glob import glob as Glob\n",
    "import json\n",
    "%matplotlib inline\n",
    "\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1 to handle images \n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/guy/sd1tb/datasets/fashion-dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/nfs/iil/home/gkoren2/local/study/git/guyk1971/PyTorch-VAE/datasets/fashion_dataset.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bai-lab-idc-gpu15.iil.intel.com/nfs/iil/home/gkoren2/local/study/git/guyk1971/PyTorch-VAE/datasets/fashion_dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/guy/sd1tb/datasets/fashion-dataset\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bai-lab-idc-gpu15.iil.intel.com/nfs/iil/home/gkoren2/local/study/git/guyk1971/PyTorch-VAE/datasets/fashion_dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m os\u001b[39m.\u001b[39;49mlistdir(dataset_path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/guy/sd1tb/datasets/fashion-dataset'"
     ]
    }
   ],
   "source": [
    "dataset_path = '/home/guy/sd1tb/datasets/fashion-dataset'\n",
    "\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2 to handle images\n",
    "import cv2\n",
    "def plot_figures(figures, nrows = 1, ncols=1,figsize=(8, 8)):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows,figsize=figsize)\n",
    "    for ind,title in enumerate(figures):\n",
    "        axeslist.ravel()[ind].imshow(cv2.cvtColor(figures[title], cv2.COLOR_BGR2RGB))\n",
    "        axeslist.ravel()[ind].set_title(title)\n",
    "        axeslist.ravel()[ind].set_axis_off()\n",
    "    plt.tight_layout() # optional\n",
    "    \n",
    "def img_path(img):\n",
    "    return dataset_path+\"/images/\"+img\n",
    "\n",
    "def load_image(img, resized_fac = 0.1):\n",
    "    img     = cv2.imread(img_path(img))\n",
    "    w, h, _ = img.shape\n",
    "    resized = cv2.resize(img, (int(h*resized_fac), int(w*resized_fac)), interpolation = cv2.INTER_AREA)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "      <td>15970.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "      <td>39386.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "      <td>59263.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21379</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Manchester United Men Solid Black Track Pants</td>\n",
       "      <td>21379.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "      <td>53759.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1855</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Inkfruit Mens Chain Reaction T-shirt</td>\n",
       "      <td>1855.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30805</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Green</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Ethnic</td>\n",
       "      <td>Fabindia Men Striped Green Shirt</td>\n",
       "      <td>30805.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26960</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Purple</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Jealous 21 Women Purple Shirt</td>\n",
       "      <td>26960.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29114</td>\n",
       "      <td>Men</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Socks</td>\n",
       "      <td>Socks</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Pack of 3 Socks</td>\n",
       "      <td>29114.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30039</td>\n",
       "      <td>Men</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Black</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Skagen Men Black Watch</td>\n",
       "      <td>30039.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
       "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
       "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
       "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
       "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
       "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
       "5   1855    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
       "6  30805    Men        Apparel     Topwear       Shirts      Green  Summer   \n",
       "7  26960  Women        Apparel     Topwear       Shirts     Purple  Summer   \n",
       "8  29114    Men    Accessories       Socks        Socks  Navy Blue  Summer   \n",
       "9  30039    Men    Accessories     Watches      Watches      Black  Winter   \n",
       "\n",
       "     year   usage                             productDisplayName      image  \n",
       "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  15970.jpg  \n",
       "1  2012.0  Casual             Peter England Men Party Blue Jeans  39386.jpg  \n",
       "2  2016.0  Casual                       Titan Women Silver Watch  59263.jpg  \n",
       "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  21379.jpg  \n",
       "4  2012.0  Casual                          Puma Men Grey T-shirt  53759.jpg  \n",
       "5  2011.0  Casual           Inkfruit Mens Chain Reaction T-shirt   1855.jpg  \n",
       "6  2012.0  Ethnic               Fabindia Men Striped Green Shirt  30805.jpg  \n",
       "7  2012.0  Casual                  Jealous 21 Women Purple Shirt  26960.jpg  \n",
       "8  2012.0  Casual                       Puma Men Pack of 3 Socks  29114.jpg  \n",
       "9  2016.0  Casual                         Skagen Men Black Watch  30039.jpg  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_path + \"/styles.csv\",on_bad_lines='skip')\n",
    "df['image'] = df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44424, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 5,\n",
       " 'masterCategory': 7,\n",
       " 'subCategory': 45,\n",
       " 'articleType': 143,\n",
       " 'baseColour': 46,\n",
       " 'season': 4,\n",
       " 'usage': 8}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols=['gender','masterCategory','subCategory','articleType','baseColour','season','usage']\n",
    "\n",
    "{c:len(df[c].value_counts()) for c in cat_cols}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are some categorical variables with many possible values . specifically `articleType`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn=[i for i in Glob(f'{dataset_path}/images/*.jpg',recursive=True)]\n",
    "len(img_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsz = [Image.open(im).size for im in img_fn]\n",
    "\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(imsz).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find image with a given size\n",
    "im=[img_fn[i] for i in range(len(img_fn)) if imsz[i] == (480,640) ]\n",
    "Image.open(im[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize the images with a limit on each axis size, while maintaining aspect ratio\n",
    "size=(256,256)\n",
    "imgs = [Image.open(im) for im in img_fn]\n",
    "[i.thumbnail(size) for i in imgs]\n",
    "[im.size for im in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of a dictionary of (title, images)\n",
    "figures = {'im'+str(i): load_image(row.image) for i, row in df.sample(6).iterrows()}\n",
    "# plot of the images in a figure, with 2 rows and 3 columns\n",
    "plot_figures(figures, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "The goal is to prepare a dataset class and dataloader that we can use to train the VAE.\n",
    "we can use the code from the `Text2Human` repo\n",
    "\n",
    "here's another [repo](https://github.com/ihciah/deep-fashion-retrieval) that uses the datasets.\n",
    "\n",
    "and here's another repo from the authors of DeepFashion: [mmfashion](https://github.com/open-mmlab/mmfashion)\n",
    "in that repo, look at the [In_shop.py](https://github.com/open-mmlab/mmfashion/blob/master/mmfashion/datasets/In_shop.py) file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Normalization\n",
    "in order to normalize the dataset, we could either use the pytorch official normalization statistics as derived from ImageNet (see [this discussion](https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2)):  \n",
    "`mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])` \n",
    "\n",
    "but we might want to compute the statistics by ourselves as the images in this dataset are not that natural. they all have a clean background.\n",
    "for that, lets define the dataset class without the normalization and then compute its stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = T.Compose([\n",
    "#     T.ToPILImage(),\n",
    "    #---------comment out data augmentation------\n",
    "    # T.ColorJitter(brightness=(0.8,1.2), \n",
    "    # contrast=(0.8,1.2), \n",
    "    # saturation=(0.8,1.2), \n",
    "    # hue=0.25),\n",
    "    # T.RandomAffine(5, translate=(0.01,0.1)),\n",
    "    #---------------------------------------------\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "    #             std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_tfms = T.Compose([\n",
    "#     T.ToPILImage(),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "    #             std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "class DeepFashion(Dataset):\n",
    "    \"\"\"DeepFashion dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        fpath = self.files[ix]\n",
    "#         clss = fname(parent(fpath))  # todo: read the label/attributes\n",
    "        img = read_image(fpath)/255.0\n",
    "        # return a dummy label at this stage as it doesnt matter\n",
    "        return img,1\n",
    "\n",
    "    def choose(self):\n",
    "        return self[np.random.randint(len(self))]\n",
    "\n",
    "    # def collate_fn(self, batch):\n",
    "    #     imgs, classes = list(zip(*batch))\n",
    "    #     if self.transform:\n",
    "    #         imgs = [self.transform(img)[None] for img in imgs]\n",
    "    #     classes = [torch.tensor([id2int[clss]]) for clss in classes]\n",
    "    #     imgs, classes = [torch.cat(i).to(device) for i in [imgs, classes]]\n",
    "    #     return imgs, classes\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        imgs, classes = list(zip(*batch))\n",
    "        if self.transform:\n",
    "            imgs = [self.transform(img)[None] for img in imgs]\n",
    "        return imgs\n",
    "\n",
    "\n",
    "dataset = DeepFashion(img_fn,trn_tfms)\n",
    "loader = DataLoader(dataset,batch_size=10,num_workers=0,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(loader))\n",
    "b,c,h,w=next(iter(loader))[0].shape\n",
    "print(b,c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(batch[0][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once we have the dataset defined we can compute the stats as follows ([reference](https://discuss.pytorch.org/t/computing-the-mean-and-std-of-dataset/34949))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset statistics. to be used for normalizing the dataset\n",
    "# h,w=next(iter(loader))[0].shape[2:]\n",
    "mean = 0.0\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0) \n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "mean = mean / len(loader.dataset)\n",
    "\n",
    "var = 0.0\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    var += ((images - mean.unsqueeze(1))**2).sum([0,2])\n",
    "std = torch.sqrt(var / (len(loader.dataset)*h*w))\n",
    "\n",
    "\n",
    "print(f'mean:{mean},std:{std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class definition\n",
    "now that we have the dataset statistics we can create a dataset that normalizes appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(256),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=[0.8323, 0.8108, 0.8040], \n",
    "    #             std=[0.2332, 0.2500, 0.2564]),\n",
    "])\n",
    "\n",
    "val_tfms = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(256),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=[0.8323, 0.8108, 0.8040], \n",
    "    #             std=[0.2332, 0.2500, 0.2564]),\n",
    "])\n",
    "\n",
    "class DeepFashion(Dataset):\n",
    "    \"\"\"DeepFashion dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path, split='train',transform=None):\n",
    "        self.files=[i for i in Glob(f'{dataset_path}/Img/**/*.jpg',recursive=True)] \n",
    "        # read item attributes:\n",
    "        # the attribute can be the cloth attribute or it can be viewing angle or type of item (based on the folder)\n",
    "        # in the following example, we're taking the cloth attributes (463 attributes)\n",
    "        anno_filename = os.path.join(dataset_path,'Anno','attributes','list_attr_items.txt')\n",
    "        with open(anno_filename,'r') as f:\n",
    "            attr_items=f.read().splitlines()\n",
    "        attr_items=attr_items[2:]\n",
    "        attr_items=[i.split() for i in attr_items]\n",
    "        self.attr_items = {i[0]:[int(int(a)>0) for a in i[1:]] for i in attr_items}\n",
    "        # get attributes names\n",
    "        anno_filename = os.path.join(dataset_path,'Anno','attributes','list_attr_cloth.txt')\n",
    "        with open(anno_filename,'r') as f:\n",
    "            attr_cloth=f.read().splitlines()\n",
    "        self.attr_names = attr_cloth[2:]\n",
    "\n",
    "        # split the dataset and throw the irrelevant part\n",
    "        train_fraction=0.8\n",
    "        train_size = int(train_fraction* len(self.attr_items.keys()))\n",
    "\n",
    "        if split=='train':\n",
    "            self.files = self.files[:train_size]\n",
    "        else:\n",
    "            self.files = self.files[train_size:]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        fpath = self.files[ix]\n",
    "        lbl=self.attr_items[fpath.split('/')[-2]]\n",
    "        img = read_image(fpath)/255.0\n",
    "        return img,lbl\n",
    "\n",
    "    def choose(self):\n",
    "        return self[np.random.randint(len(self))]\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        imgs, attrs = list(zip(*batch))\n",
    "        if self.transform:\n",
    "            imgs = [self.transform(img)[None] for img in imgs]\n",
    "        else:\n",
    "            imgs = [img[None] for img in imgs]\n",
    "        attrs=[torch.tensor(a)[None] for a in attrs]\n",
    "        imgs,attrs = [torch.cat(i) for i in [imgs,attrs]]\n",
    "        return imgs,attrs\n",
    "\n",
    "dataset = DeepFashion(dataset_path,'train')\n",
    "loader = DataLoader(dataset,batch_size=10,num_workers=0,shuffle=False,collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(loader))\n",
    "b,c,h,w=batch[0].shape\n",
    "print(b,c,h,w)\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(batch[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CelebA\n",
    "this is the dataset considered as a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clba_path = '/home/guy/sd1tb/datasets/CelebA/celeba/img_align_celeba'\n",
    "# clba_path = '/data/users/gkoren2/datasets/celeba/celeba/img_align_celeba'   # gpu15\n",
    "imgs_filenames=os.listdir(clba_path)\n",
    "len(imgs_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract image sizes and make sure they all at sime size\n",
    "imsz = [Image.open(os.path.join(clba_path,im)).size for im in imgs_filenames]\n",
    "np.all([s==imsz[0] for s in imsz])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the alignment by displaying the average of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_indxs = [i for i in range(1000)]\n",
    "avg_img = np.zeros_like(Image.open(os.path.join(clba_path,imgs_filenames[0])),dtype=np.float64)\n",
    "for i,idx in enumerate(img_indxs):\n",
    "    img=Image.open(os.path.join(clba_path,imgs_filenames[idx]))\n",
    "    avg_img+=np.asarray(img)\n",
    "avg_img = avg_img/len(img_indxs)\n",
    "Image.fromarray(np.uint8(avg_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, we see that the images are pretty much aligned. this is something that should predict the successfulness of the VAE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make a gif from the images:\n",
    "img_indxs = [i for i in range(1000)]\n",
    "imgs = (Image.open(os.path.join(clba_path,imgs_filenames[i])) for i in img_indxs)\n",
    "img = next(imgs)  # extract first image from iterator\n",
    "img.save(fp='celeba.gif', format='GIF', append_images=imgs,save_all=True, duration=20, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load CelebA dataset class\n",
    "Lets import the dataset as it is done in the VAE s.t. we understand how to prepare `DeepFashion` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_check_integrity',\n",
       " '_format_transform_repr',\n",
       " '_is_protocol',\n",
       " '_load_csv',\n",
       " '_repr_indent',\n",
       " 'attr',\n",
       " 'attr_names',\n",
       " 'base_folder',\n",
       " 'bbox',\n",
       " 'download',\n",
       " 'extra_repr',\n",
       " 'file_list',\n",
       " 'filename',\n",
       " 'identity',\n",
       " 'landmarks_align',\n",
       " 'root',\n",
       " 'split',\n",
       " 'target_transform',\n",
       " 'target_type',\n",
       " 'transform',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import CelebA\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms as T\n",
    "# clba_path = '/data/users/gkoren2/datasets/celeba/'   # gpu15\n",
    "clba_path = '/home/guy/sd1tb/datasets/CelebA/'\n",
    "patch_size=256\n",
    "trn_tfms = T.Compose([      # copied from dataset.py \n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.CenterCrop(148),\n",
    "    T.Resize(patch_size),\n",
    "    T.ToTensor(),\n",
    "    ])\n",
    "\n",
    "clba = CelebA(clba_path, split='train',transform=trn_tfms,download=False)\n",
    "\n",
    "dir(clba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(clba,'collate_fn',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clba_loader = data.DataLoader(clba,batch_size=10,num_workers=0,shuffle=False)\n",
    "clba_batch=next(iter(clba_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clba_batch[0].shape)       # the images tensor\n",
    "print(clba_batch[1].shape)       # the labels (attributes) tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['5_o_Clock_Shadow',\n",
       " 'Arched_Eyebrows',\n",
       " 'Attractive',\n",
       " 'Bags_Under_Eyes',\n",
       " 'Bald',\n",
       " 'Bangs',\n",
       " 'Big_Lips',\n",
       " 'Big_Nose',\n",
       " 'Black_Hair',\n",
       " 'Blond_Hair',\n",
       " 'Blurry',\n",
       " 'Brown_Hair',\n",
       " 'Bushy_Eyebrows',\n",
       " 'Chubby',\n",
       " 'Double_Chin',\n",
       " 'Eyeglasses',\n",
       " 'Goatee',\n",
       " 'Gray_Hair',\n",
       " 'Heavy_Makeup',\n",
       " 'High_Cheekbones',\n",
       " 'Male',\n",
       " 'Mouth_Slightly_Open',\n",
       " 'Mustache',\n",
       " 'Narrow_Eyes',\n",
       " 'No_Beard',\n",
       " 'Oval_Face',\n",
       " 'Pale_Skin',\n",
       " 'Pointy_Nose',\n",
       " 'Receding_Hairline',\n",
       " 'Rosy_Cheeks',\n",
       " 'Sideburns',\n",
       " 'Smiling',\n",
       " 'Straight_Hair',\n",
       " 'Wavy_Hair',\n",
       " 'Wearing_Earrings',\n",
       " 'Wearing_Hat',\n",
       " 'Wearing_Lipstick',\n",
       " 'Wearing_Necklace',\n",
       " 'Wearing_Necktie',\n",
       " 'Young',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(clba.attr_names))\n",
    "clba.attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clba_batch[0].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like the only normalization they have applied is scaling s.t. the value of the pixels are in [0,1]. they havent normalized it to `mean=0 , stdev=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()(clba_batch[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clba_batch[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clba_batch[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFashion dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Shop Clothes Retrieval\n",
    "\n",
    "- Images (Img/img.zip):   52,712 in-shop clothes images (~200,000 cross-pose/scale pairs).\n",
    "    - Images are centered and resized to 256*256;\n",
    "    - The aspect ratios of original images are kept unchanged\n",
    "- Bounding Box Annotations (Anno/list_bbox_inshop.txt)\n",
    "- Fashion Landmark Annotations (Anno/list_landmarks_inshop.txt)\n",
    "- Item Annotations (Anno/list_item_inshop.txt)\n",
    "- Description Annotations (Anno/list_description_inshop.json)\n",
    "- Attribute Annotations (Anno/attributes/list_attr_cloth.txt & Anno/attributes/list_attr_items.txt & Anno/attributes/list_color_cloth.txt)\n",
    "- Segmentation Mask Annotations (Anno/segmentation/DeepFashion_instances_train.json & Anno/segmentation/DeepFashion_instances_query.json & Anno/segmentation/DeepFashion_instances_gallery.json)\n",
    "- Dense Pose Annotations (Anno/densepose/img_iuv.zip)\n",
    "- Evaluation Partitions (Eval/list_eval_partition.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/users/gkoren2/datasets/DeepFashion/In-shop-Clothes-Retrieval/In-shop Clothes Retrieval Benchmark' # gpu15\n",
    "# benchmarks = [os.path.basename(bm) for bm in Glob(dataset_path+'/*') if os.path.isdir(bm)]\n",
    "# benchmarks\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_filename = os.path.join(dataset_path,'Anno','list_item_inshop.txt')\n",
    "with open(anno_filename,'r') as f:\n",
    "    item_list=f.read().splitlines()\n",
    "\n",
    "len(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_filename = os.path.join(dataset_path,'Anno','list_description_inshop.json')\n",
    "with open(anno_filename,'r') as f:\n",
    "    item_desc = json.load(f)\n",
    "desc_df=pd.DataFrame(item_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df.item.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df.loc[desc_df['item']=='id_00000001',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_filename = os.path.join(dataset_path,'Anno','attributes','list_attr_cloth.txt')\n",
    "with open(anno_filename,'r') as f:\n",
    "    attr_cloth=f.read().splitlines()\n",
    "attr_cloth = attr_cloth[2:]\n",
    "len(attr_cloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_cloth[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_filename = os.path.join(dataset_path,'Anno','attributes','list_attr_items.txt')\n",
    "with open(anno_filename,'r') as f:\n",
    "    attr_items=f.read().splitlines()\n",
    "attr_items=attr_items[2:]\n",
    "attr_items=[i.split() for i in attr_items]\n",
    "attr_items = {i[0]:[int(int(a)>0) for a in i[1:]] for i in attr_items}\n",
    "len(attr_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(attr_items['id_00000013']))\n",
    "attr_items['id_00000013']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. The order of attribute labels accords with the order of attribute names;\n",
    "2. In attribute labels, \"1\" represents positive while \"-1\" represents negative, '0' represents unknown;\n",
    "3. Attribute prediction is treated as a multi-label tagging problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_item_desc={i[0]:[attr_cloth[ai] for ai in range(len(attr_cloth)) if i[ai+1]=='1'] for i in attr_items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_item_desc['id_00000013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_filename = os.path.join(dataset_path,'Anno','attributes','list_color_cloth.txt')\n",
    "with open(anno_filename,'r') as f:\n",
    "    color_cloth=f.read().splitlines()\n",
    "\n",
    "len(color_cloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_cloth[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_part_filename = os.path.join(dataset_path,'Eval','list_eval_partition.txt')\n",
    "with open(eval_part_filename,'r') as f:\n",
    "    eval_part_list=f.read().splitlines()\n",
    "eval_part_list=eval_part_list[2:]\n",
    "eval_part_list=[i.split() for i in eval_part_list]\n",
    "eval_part_list = {i[1]:i[2] for i in eval_part_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(eval_part_list.values()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epl0=eval_part_list[2]\n",
    "epl0.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tree \"{os.path.join(dataset_path,'Img/img')}\" -L 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn=[i for i in Glob(f'{dataset_path}/Img/**/*.jpg',recursive=True)] #  if 'side' in i]\n",
    "len(img_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base name\n",
    "img_bn=[s.split('img')[1] for s in img_fn]\n",
    "len(img_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in img_fn if 'id_00003321' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tree \"{os.path.join(os.path.dirname(dataset_path),'img_highres')}\" -L 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hres_fn = [i for i in Glob(f\"{os.path.join(os.path.dirname(dataset_path),'img_highres')}/**/*.jpg\",recursive=True)]\n",
    "len(img_hres_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hres_fn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hres_bn=[s.split('img_highres')[1] for s in img_hres_fn]\n",
    "len(img_hres_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comapre the names - make sure we have exact copy of each image as highres\n",
    "np.all(np.array(img_bn)==np.array(img_hres_bn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see an image and its highres variant\n",
    "img_id = 2345\n",
    "img = Image.open(img_fn[img_id])\n",
    "img_hres=Image.open(img_hres_fn[img_id])\n",
    "print(f'{img.size} , {img_hres.size}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll ignore the high res images. look only at the low res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get item images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn=[i for i in Glob(f'{dataset_path}/Img/**/*.jpg',recursive=True)] #  if 'side' in i]\n",
    "img_hres_fn = [i for i in Glob(f\"{os.path.join(os.path.dirname(dataset_path),'img_highres')}/**/*.jpg\",recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_images(item_id,hres=False):\n",
    "    img_fns=img_hres_fn if hres else img_fn\n",
    "    img_filenames = [f for f in img_fns if item_id in f]\n",
    "    if len(img_filenames)==0:\n",
    "        print(f'item {item_id} not found')\n",
    "        return None\n",
    "    else:\n",
    "        imgs = {os.path.basename(f):Image.open(f) for f in img_filenames}\n",
    "        print(f'found {len(imgs)} for item {item_id}')\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = get_item_images('id_00000002',False)\n",
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(list(imgs.values()),2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image stats\n",
    "img_indxs = [i for i in range(len(img_fn))]\n",
    "avg_img = np.zeros_like(Image.open(img_fn[0]),dtype=np.float64)\n",
    "for i,idx in enumerate(img_indxs):\n",
    "    img=Image.open(img_fn[idx])\n",
    "    avg_img+=np.asarray(img)\n",
    "avg_img = avg_img/len(img_indxs)\n",
    "# Image.fromarray(np.uint8(avg_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn[0].split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_img[:,:,0].mean()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai1=avg_img.reshape([-1,3]).mean(axis=0)/255\n",
    "ai1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai1-np.array([0.485, 0.456, 0.406])/np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category and Attribute Prediction\n",
    "\n",
    "- Images (Img/img.zip):  289,222 diverse clothes images. \n",
    "    - The long side of images are resized to 300;\n",
    "    - The aspect ratios of original images are kept unchanged.\n",
    "- Bounding Box Annotations (Anno/list_bbox.txt) bounding box labels. \n",
    "- Fashion Landmark Annotations (Anno/list_landmarks.txt) fashion landmark labels\n",
    "- Category Annotations (Anno/list_category_cloth.txt & Anno/list_category_img.txt) clothing category labels. \n",
    "- Attribute Annotations (Anno/list_attr_cloth.txt & Anno/list_attr_img.txt) clothing attribute labels. \n",
    "- Evaluation Partitions (Eval/list_eval_partition.txt) image names for training, validation and testing set respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/users/gkoren2/datasets/DeepFashion/Category-and-Attribute-Prediction' # gpu15\n",
    "# benchmarks = [os.path.basename(bm) for bm in Glob(dataset_path+'/*') if os.path.isdir(bm)]\n",
    "# benchmarks\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn=[i for i in Glob(f'{dataset_path}/img/**/*.jpg',recursive=True)] #  if 'side' in i]\n",
    "img_hres_fn = [i for i in Glob(f\"{os.path.join(dataset_path,'img_highres')}/**/*.jpg\",recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(img_fn))\n",
    "print(len(img_hres_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(os.path.dirname(img_fn[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=[Image.open(f) for f in img_fn[100:110]]\n",
    "image_grid(imgs,2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/users/gkoren2/datasets/DeepFashion/In-shop-Clothes-Retrieval' # gpu15\n",
    "# dataset_path = '/home/guy/sd1tb/datasets/deep_fashion/DeepFashion' # guy-x\n",
    "benchmarks = [os.path.basename(bm) for bm in Glob(dataset_path+'/*') if os.path.isdir(bm)]\n",
    "benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn=[i for i in Glob(f'{dataset_path}/{benchmarks[0]}/Img/**/*.jpg',recursive=True)] #  if 'side' in i]\n",
    "len(img_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsz = [Image.open(im).size for im in img_fn]\n",
    "df=pd.DataFrame(imsz,columns=['x','y'])\n",
    "df['new_col'] = list(zip(df.x, df.y))\n",
    "df['img_fn']=img_fn\n",
    "df['new_col'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the smallest image in the dataset\n",
    "# img=Image.open(df.loc[df['x']==70,'img_fn'])\n",
    "img=Image.open(df.loc[df['x']==df['x'].min(),'img_fn'].values[0])\n",
    "print(img.size)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x'].sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=[Image.open(m) for m in img_fn[200:225]]\n",
    "image_grid(imgs,5,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the average image \n",
    "img_indxs = [i for i in range(len(img_fn))]\n",
    "avg_img = np.zeros_like(Image.open(img_fn[0]),dtype=np.float64)\n",
    "for i,idx in enumerate(img_indxs):\n",
    "    img=Image.open(img_fn[idx])\n",
    "    avg_img+=np.asarray(img)\n",
    "avg_img = avg_img/len(img_indxs)\n",
    "Image.fromarray(np.uint8(avg_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the average image looks poor. it would be interesting to see whether the VAE can capture the variance in the dataset and get farther from the average image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, lets start with the `In-Shop-Clothes-Retreival-Benchmark` dataset and try to find a disentangled representation using VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/users/gkoren2/datasets/DeepFashion/In-shop-Clothes-Retrieval/In-shop Clothes Retrieval Benchmark' # gpu15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "things we'll need to do:\n",
    "- transform to tensor\n",
    "- normalize the dataset\n",
    "\n",
    "we wont do any further adjustments or augmentations at this stage.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ptvae')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "56681f32b047566157fb60ffac65a2814c6eb720e5ca2d0a1d28762cefafd161"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
